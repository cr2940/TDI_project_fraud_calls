{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DO NOT CALL Numbers\n",
    "\n",
    "## The following data is from FTC.gov:\n",
    "\n",
    "https://www.ftc.gov/site-information/open-government/data-sets/do-not-call-data\n",
    "\n",
    "Here we look at some common area codes that affect different states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File dnc_complaint_numbers_2021-10-06.csv does not exist: 'dnc_complaint_numbers_2021-10-06.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-8fc2ee8db919>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Do Not Call numbers complaint reports taken from 9/20/21 - 10/6/21  (~ 18 MB)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dnc_complaint_numbers_2021-10-06.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dnc_complaint_numbers_2021-10-05.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0mdf2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dnc_complaint_numbers_2021-10-04.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0mdf3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dnc_complaint_numbers_2021-10-01.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0mdf4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File dnc_complaint_numbers_2021-10-06.csv does not exist: 'dnc_complaint_numbers_2021-10-06.csv'"
     ]
    }
   ],
   "source": [
    "# Do Not Call numbers complaint reports taken from 9/20/21 - 10/6/21  (~ 18 MB)\n",
    "df = pd.read_csv('dnc_complaint_numbers_2021-10-06.csv');df\n",
    "df2 = pd.read_csv('dnc_complaint_numbers_2021-10-05.csv');df2 \n",
    "df3 = pd.read_csv('dnc_complaint_numbers_2021-10-04.csv');df3\n",
    "df4 = pd.read_csv('dnc_complaint_numbers_2021-10-01.csv');df4\n",
    "df5 = pd.read_csv('dnc_complaint_numbers_2021-09-27.csv');df4\n",
    "df6 = pd.read_csv('dnc_complaint_numbers_2021-09-28.csv');df4\n",
    "df7 = pd.read_csv('dnc_complaint_numbers_2021-09-24.csv');df4\n",
    "df8 = pd.read_csv('dnc_complaint_numbers_2021-09-23.csv');df4\n",
    "df9 = pd.read_csv('dnc_complaint_numbers_2021-09-22.csv');df4\n",
    "df10 = pd.read_csv('dnc_complaint_numbers_2021-09-21.csv');df4\n",
    "df11 = pd.read_csv('dnc_complaint_numbers_2021-09-20.csv');df4\n",
    "\n",
    "# All Concatenated Togeter\n",
    "df_oct_wk = pd.concat([df,df2,df3,df4,df5,df6,df7,df8,df9,df10,df11])\n",
    "df_oct_wk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Data Cleaning: Very messy data due to human reporting error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    # for some reason running the code once does not get rid of empty string row, but running it twice does:\n",
    "    df_oct_wk = df_oct_wk[df_oct_wk[\"Company_Phone_Number\"]!='']\n",
    "    \n",
    "    # cleaning data, getting rid of nan, alphabetical entries, special characters, bytestrings\n",
    "    df_oct_wk = df_oct_wk.dropna()\n",
    "    print(df_oct_wk[\"Company_Phone_Number\"])\n",
    "    df_oct_wk = df_oct_wk[~df_oct_wk[\"Company_Phone_Number\"].str.isalpha()]\n",
    "    alphabets=['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',\\\n",
    "              'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
    "    for c in alphabets:\n",
    "        df_oct_wk = df_oct_wk[~df_oct_wk[\"Company_Phone_Number\"].str.contains(c)]\n",
    "    print(df_oct_wk[\"Company_Phone_Number\"])\n",
    "    replacement_dict = {'–':'','a':'','b':'','c':'','d':'','e':'','f':'','g':'', \\\n",
    "                                                  'h':'','i':'','k':'','l':'','m':'','n':'','o':'','p':'','q':'','r':'', \\\n",
    "                                                  's':'','t':'','u':'','v':'','w':'','x':'','y':'','z':'','!':'', \\\n",
    "                                                  '@':'','#':'','$':'','%':'','^':'','&':'','*':'','(':'',')':'','?':'',\\\n",
    "                       'A':'','B':'','C':'','D':'','E':'','X':'','Y':'','Z':'','F':'','\\xa0':'','=':'',';':'','`':'','´':''}\n",
    "    for i, (k, v) in enumerate(replacement_dict.items()):\n",
    "        df_oct_wk[\"Company_Phone_Number\"]=df_oct_wk[\"Company_Phone_Number\"].str.replace(k,v,regex=False)\n",
    "    \n",
    "\n",
    "# make area code column\n",
    "df_oct_wk[\"Scam AreaCode\"] = round(df_oct_wk[\"Company_Phone_Number\"].astype(float)/10**7)\n",
    "df_oct_wk[\"Scam AreaCode\"] = df_oct_wk[\"Scam AreaCode\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Coordinates Data for Mapping Later: \n",
    "\n",
    "https://raw.githubusercontent.com/jasperdebie/VisInfo/master/us-state-capitals.csv\n",
    "\n",
    "https://worldpopulationreview.com/states/state-abbreviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_coords_url =\"https://raw.githubusercontent.com/jasperdebie/VisInfo/master/us-state-capitals.csv\"\n",
    "df_st_co = pd.read_csv(states_coords_url)\n",
    "df_st = pd.read_csv('./state_code.csv')\n",
    "df_st_coord_code = pd.merge(df_st_co,df_st[[\"State\",\"Code\"]],how='left',left_on='name',right_on='State')\n",
    "df_st_coord_code = df_st_coord_code.drop(columns=\"State\")\n",
    "df_st_coords = df_st_coord_code[[\"Code\",\"latitude\",\"longitude\"]]; \n",
    "\n",
    "# add DC:\n",
    "DC_row = pd.Series({\"Code\":\"DC\",\"latitude\":38.9072,\"longitude\":77.0369},name=50)\n",
    "df_st_coords = df_st_coords.append(DC_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Area Code by State --> Coordinates by Area Code\n",
    "\n",
    "## Data for mapping between area code and state: from North American Numbering Plan Administrator (NANPA)\n",
    "\n",
    "https://www.nationalnanpa.com/enas/geoAreaCodeNumberReport.do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_areaco_st = pd.read_excel('./area_code_state.xlsx')\n",
    "df_area_co_st_coord = pd.merge(df_areaco_st,df_st_coords,how=\"left\",left_on=\"Location\",right_on=\"Code\")\n",
    "df_area_co_st_coord = df_area_co_st_coord.dropna();\n",
    "df_area_co_st_coord = df_area_co_st_coord.drop(columns='Location')\n",
    "df_area_co_st_coord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where are the spammers and scammers calling from?\n",
    "\n",
    "## Similarity by proximity?\n",
    "\n",
    "We want to see if there are similar scammer area codes that call nearby regions, so we selected GA and AL, and NY and NJ. Unfortunately, each state has most number of scam calls from its own state. However, we found that the most common area code number is from different states.\n",
    "\n",
    "In fact, one state, NJ, has the most spammers and scammers from area code 974 , which is unidentifiable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most common scam number in Georgia\n",
    "scams_GA = df_oct_wk[df_oct_wk[\"Consumer_State\"]==\"Georgia\"]\n",
    "scams_GA = scams_GA[(scams_GA[\"Scam AreaCode\"]>100) & (scams_GA[\"Scam AreaCode\"]<1000)]\n",
    "scams_GA_AREA = scams_GA[\"Scam AreaCode\"]\n",
    "mostcom = scams_GA_AREA.mode()\n",
    "\n",
    "scams_GA_AREA.plot.hist()\n",
    "plt.title(\"Histogram of area codes of scam calls in Georgia\")\n",
    "print(\"The most common area code that is on the spam list for Georgia is \", int(mostcom), \"which is from California\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further analysis on scam call numbers:\n",
    "\n",
    "Although the most common area code is 707, which is a California area code, we see that there are aggregates of area codes which all correspond to the state of Georgia and sum up to be in greater occurrence than the CA area code, although each appears less in frequency than the CA area code. Nonetheless, there is a great number of callers from CA numbers that spam and scam GA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_cts_GA = scams_GA_AREA.value_counts().rename_axis('areacodes').reset_index(name='counts');(val_cts_GA)\n",
    "GA_area_coords = pd.merge(val_cts_GA,df_area_co_st_coord,how=\"left\",left_on=\"areacodes\",right_on=\"NPA\");\n",
    "GA_area_coords = GA_area_coords.dropna(); \n",
    "GA_area_coords = GA_area_coords.groupby(['Code','latitude','longitude']).agg({'counts':'sum'})\n",
    "\n",
    "GA_area_coords['text'] = \"<br>Number of callers: \" +GA_area_coords['counts'].astype(str)\n",
    "GA_area_coords = GA_area_coords.reset_index()\n",
    "import plotly.graph_objects as go\n",
    "fig = go.Figure(data=go.Scattergeo(\n",
    "        lon = GA_area_coords['longitude'],\n",
    "        lat = GA_area_coords['latitude'],\n",
    "        mode = 'markers',\n",
    "       marker = dict(\n",
    "            size = GA_area_coords['counts']/10),\n",
    "        text = GA_area_coords['text']\n",
    "        ))\n",
    "\n",
    "fig.update_layout(\n",
    "        title = 'Where the scammers are from for Georgia',\n",
    "        geo_scope='usa',\n",
    "    )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most common scam number in Alabama\n",
    "scams_AL = df_oct_wk[df_oct_wk[\"Consumer_State\"]==\"Alabama\"]\n",
    "scams_AL = scams_AL[(scams_AL[\"Scam AreaCode\"]>100) & (scams_AL[\"Scam AreaCode\"]<1000)]\n",
    "scams_AL_AREA = scams_AL[\"Scam AreaCode\"]\n",
    "mostcom = scams_AL_AREA.mode()\n",
    "\n",
    "scams_AL_AREA.hist()\n",
    "plt.title(\"Histogram of area codes of scam calls in Alabama\")\n",
    "print(\"The most common area code that is on the spam list for Alabama is \", int(mostcom), \"which is from Washington\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_cts_AL = scams_AL_AREA.value_counts().rename_axis('areacodes').reset_index(name='counts');\n",
    "AL_area_coords = pd.merge(val_cts_AL,df_area_co_st_coord,how=\"left\",left_on=\"areacodes\",right_on=\"NPA\");\n",
    "AL_area_coords = AL_area_coords.dropna(); \n",
    "AL_area_coords = AL_area_coords.groupby(['Code','latitude','longitude']).agg({'counts':'sum'})\n",
    "\n",
    "AL_area_coords['text'] = \"<br>Number of callers: \" +AL_area_coords['counts'].astype(str)\n",
    "AL_area_coords = AL_area_coords.reset_index()\n",
    "import plotly.graph_objects as go\n",
    "fig = go.Figure(data=go.Scattergeo(\n",
    "        lon = AL_area_coords['longitude'],\n",
    "        lat = AL_area_coords['latitude'],\n",
    "        mode = 'markers',\n",
    "       marker = dict(\n",
    "            size = AL_area_coords['counts']/10),\n",
    "        text = AL_area_coords['text']\n",
    "        ))\n",
    "\n",
    "fig.update_layout(\n",
    "        title = 'Where the scammers are from for Alabama',\n",
    "        geo_scope='usa',\n",
    "    )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most common scam number in New York\n",
    "scams_NY = df_oct_wk[df_oct_wk[\"Consumer_State\"]==\"New York\"]\n",
    "scams_NY = scams_NY[(scams_NY[\"Scam AreaCode\"]>100) & (scams_NY[\"Scam AreaCode\"]<1000)]\n",
    "scams_NY_AREA = scams_NY[\"Scam AreaCode\"]\n",
    "mostcom = scams_NY_AREA.mode()\n",
    "\n",
    "scams_NY_AREA.hist()\n",
    "plt.title(\"Histogram of area codes of scam calls in New York\")\n",
    "print(\"The most common area code that is on the spam list for New York is \", int(mostcom), \"which is from Colorado\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although Colorado was the most commonly appearing area code, this graph shows two things. 1) Area codes from NY when aggregated outnumber Colorado callers, and 2) area codes from Michigan are the second outnumbering calling state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_cts_NY = scams_NY_AREA.value_counts().rename_axis('areacodes').reset_index(name='counts');\n",
    "NY_area_coords = pd.merge(val_cts_NY,df_area_co_st_coord,how=\"left\",left_on=\"areacodes\",right_on=\"NPA\");\n",
    "NY_area_coords = NY_area_coords.dropna(); \n",
    "NY_area_coords =NY_area_coords.groupby(['Code','latitude','longitude']).agg({'counts':'sum'})\n",
    "\n",
    "NY_area_coords['text'] = \"<br>Number of callers: \" +NY_area_coords['counts'].astype(str)\n",
    "NY_area_coords =NY_area_coords.reset_index()\n",
    "import plotly.graph_objects as go\n",
    "fig = go.Figure(data=go.Scattergeo(\n",
    "        lon = NY_area_coords['longitude'],\n",
    "        lat = NY_area_coords['latitude'],\n",
    "        mode = 'markers',\n",
    "       marker = dict(\n",
    "            size = NY_area_coords['counts']/10),\n",
    "        text = NY_area_coords['text']\n",
    "        ))\n",
    "\n",
    "fig.update_layout(\n",
    "        title = 'Where the scammers are from for New York',\n",
    "        geo_scope='usa',\n",
    "    )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most common scam number in New Jersey\n",
    "scams_NJ = df_oct_wk[df_oct_wk[\"Consumer_State\"]==\"New Jersey\"]\n",
    "scams_NJ = scams_NJ[(scams_NJ[\"Scam AreaCode\"]>100) & (scams_NJ[\"Scam AreaCode\"]<1000)]\n",
    "scams_NJ_AREA = scams_NJ[\"Scam AreaCode\"]\n",
    "mostcom = scams_NJ_AREA.mode()\n",
    "\n",
    "scams_NJ_AREA.hist()\n",
    "plt.title(\"Histogram of area codes of scam calls in New Jersey\")\n",
    "print(\"The most common area code that is on the spam list for New Jersey is \", int(mostcom), \"which is unidentifiable\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the mode of area code is 974, an unidentifiable number, we do see that when all the area codes are aggregated, the most scam/spam calling state for NJ, is NJ itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_cts_NJ = scams_NJ_AREA.value_counts().rename_axis('areacodes').reset_index(name='counts');\n",
    "NJ_area_coords = pd.merge(val_cts_NJ,df_area_co_st_coord,how=\"left\",left_on=\"areacodes\",right_on=\"NPA\");\n",
    "NJ_area_coords =NJ_area_coords.dropna(); \n",
    "NJ_area_coords =NJ_area_coords.groupby(['Code','latitude','longitude']).agg({'counts':'sum'})\n",
    "\n",
    "NJ_area_coords['text'] = \"<br>Number of callers: \" +NJ_area_coords['counts'].astype(str)\n",
    "NJ_area_coords =NJ_area_coords.reset_index()\n",
    "import plotly.graph_objects as go\n",
    "fig = go.Figure(data=go.Scattergeo(\n",
    "        lon = NJ_area_coords['longitude'],\n",
    "        lat = NJ_area_coords['latitude'],\n",
    "        mode = 'markers',\n",
    "       marker = dict(\n",
    "            size = NJ_area_coords['counts']/10),\n",
    "        text = NJ_area_coords['text']\n",
    "        ))\n",
    "\n",
    "fig.update_layout(\n",
    "        title = 'Where the scammers are from for New Jersey',\n",
    "        geo_scope='usa',\n",
    "    )\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
